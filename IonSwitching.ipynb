{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.tabular import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv(PATH/\"train.csv\")\n",
    "# test = pd.read_csv(PATH/\"test.csv\")\n",
    "\n",
    "train = pd.read_csv(PATH/\"train_clean.csv\")   # data-without-drift\n",
    "test = pd.read_csv(PATH/\"test_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "IMPORTANT: While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# imbalanced dataset\n",
    "c = Counter(train.open_channels); c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i,n in c.most_common():\n",
    "    print(f\"{i}: {round(n/5000000, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# signal stats\n",
    "print(f\"mean: {train.signal.mean()}\\nmin: {train.signal.min()}\\nmax: {train.signal.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# open channel stats\n",
    "print(f\"mean: {train.open_channels.mean()}\\nmin: {train.open_channels.min()}\\nmax: {train.open_channels.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res = 10000    # step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(train.signal[0::res])\n",
    "\n",
    "plt.vlines(range(0,len(train),500000),-5,14,'r')  # delineate batches (vertically from -5 to 12.5)\n",
    "for j in range(10): plt.text(j*500000+200000,13,str(j+1),size=20)  # show batch numbers\n",
    "\n",
    "plt.xlabel('Time (*1e4)', size=16)\n",
    "plt.ylabel('Signal', size=16); \n",
    "plt.title(f'Signal - batches / {res}',size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(train.open_channels[0::res])\n",
    "\n",
    "plt.vlines(range(0,len(train),500000),-1,14,'r')  # delineate batches (vertically from -5 to 12.5)\n",
    "for j in range(10): plt.text(j*500000+200000,13,str(j+1),size=20)  # show batch numbers\n",
    "\n",
    "plt.xlabel('Time (*1e4)', size=16)\n",
    "plt.ylabel('Open Channels', size=16);\n",
    "plt.title(f'Open Channels - batches / {res}',size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(train.signal[::res])\n",
    "plt.plot(train.open_channels[::res])\n",
    "\n",
    "plt.vlines(range(0,len(train),500000),-5,14,'r')  # delineate batches (vertically from -5 to 12.5)\n",
    "for j in range(10): plt.text(j*500000+200000,13,str(j+1),size=20)  # show batch numbers\n",
    "\n",
    "plt.xlabel('Time (*1e4)', size=16)\n",
    "plt.ylabel('Signal / Channels Open', size=16); \n",
    "plt.title(f'Overlay - batches / {res}',size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## zoom in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_subsets(n=10, length=500, step=10):\n",
    "    for k in range(n):\n",
    "        start = np.random.randint(0, len(train)-length)\n",
    "        end = start + length\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.plot(train.signal[start:end:step])\n",
    "        plt.plot(train.open_channels[start:end:step])\n",
    "        plt.xlabel('Time (*1e4)', size=16)\n",
    "        plt.ylabel('Signal / Channels Open', size=16); \n",
    "        plt.title(f'{start} - {end}  ({length}/{step})',size=20)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_subsets(n=2, length=500, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(test.signal[0::res])\n",
    "\n",
    "plt.vlines(range(0,len(test),500000),-5,14,'r')  # delineate batches (vertically from -5 to 12.5)\n",
    "for j in range(4): plt.text(j*500000+200000,13,str(j+1),size=20)  # show batch numbers\n",
    "\n",
    "plt.xlabel('Time (*1e4)', size=16)\n",
    "plt.ylabel('Signal', size=16); \n",
    "plt.title(f'Signal - batches / {res}',size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 4000\n",
    "bs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "train_mean = train.signal.mean()\n",
    "train_sigma = train.signal.std()\n",
    "\n",
    "train['signal'] = (train.signal - train_mean) / train_sigma\n",
    "test['signal'] = (test.signal - train_mean) / train_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.clip is hacky way to get the std to near 1\n",
    "\n",
    "def grouping_(df, seq_len):\n",
    "    # group by seq_len\n",
    "    df['group'] = df.groupby(df.index//seq_len, sort=False)['signal'].agg(['ngroup']).values\n",
    "    df['group'] = df['group'].astype(np.uint16)\n",
    "\n",
    "    # group_mean and percent change from group_mean\n",
    "    df['group_mean'] = df.groupby('group')['signal'].transform('mean')\n",
    "    df['pct_change_group_mean'] = np.clip((df['signal']/df['group_mean'])-1,-2,2)\n",
    "    #return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_(train, seq_len)\n",
    "grouping_(test, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.clip is hacky way to get the std to near 1\n",
    "\n",
    "def shift_with_pct_change_(df, windows):\n",
    "    for window in windows:\n",
    "        df['signal_shift_prev_' + str(window)] = df['signal'].shift(window).fillna(df['signal'])\n",
    "        df['pct_change_prev_' + str(window)] = np.clip((df['signal']/df['signal_shift_prev_' + str(window)])-1,-4,4)\n",
    "        df['signal_shift_next_' + str(window)] = df['signal'].shift(-1 * window).fillna(df['signal'])\n",
    "        df['pct_change_next_' + str(window)] = np.clip((df['signal']/df['signal_shift_next_' + str(window)])-1,-4,4)\n",
    "    #return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_with_pct_change_(train, [1])\n",
    "shift_with_pct_change_(test, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(time                     0\n",
       " signal                   0\n",
       " open_channels            0\n",
       " group                    0\n",
       " group_mean               0\n",
       " pct_change_group_mean    0\n",
       " signal_shift_prev_1      0\n",
       " pct_change_prev_1        0\n",
       " signal_shift_next_1      0\n",
       " pct_change_next_1        0\n",
       " dtype: int64,\n",
       " time                     0\n",
       " signal                   0\n",
       " group                    0\n",
       " group_mean               0\n",
       " pct_change_group_mean    0\n",
       " signal_shift_prev_1      0\n",
       " pct_change_prev_1        0\n",
       " signal_shift_next_1      0\n",
       " pct_change_next_1        0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for na values\n",
    "# train.isnull().sum(), test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['signal', 'group_mean', 'pct_change_group_mean', 'pct_change_prev_1', 'pct_change_next_1']  \n",
    "\n",
    "X = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n",
    "Y = np.array(list(train.groupby('group').apply(lambda x: x['open_channels'].values)))\n",
    "\n",
    "TEST = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 4000, 6) (1250, 4000)\n",
      "(500, 4000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape)\n",
    "print(TEST.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means: [-1.754979e-14  1.754097e-13 -1.168207e-03 -3.224784e-02 -3.501341e-02]\n",
      " stds: [1.       0.889433 0.916579 1.026751 1.027284]\n"
     ]
    }
   ],
   "source": [
    "print(f\"means: {X.mean(axis=(0,1))}\")\n",
    "print(f\" stds: {X.std(axis=(0,1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent change can be approximated by log differences..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split trn/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4000, 6) (250, 4000, 6)\n"
     ]
    }
   ],
   "source": [
    "trn_idx,val_idx = trn_val_split(X.shape[0])\n",
    "\n",
    "trn_x, val_x = X[trn_idx], X[val_idx]\n",
    "trn_y, val_y = Y[trn_idx], Y[val_idx]\n",
    "print(trn_x.shape, val_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = Ion_Dataset(trn_x, trn_y)\n",
    "val_ds = Ion_Dataset(val_x, val_y)\n",
    "\n",
    "trn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=bs, shuffle=False)\n",
    "\n",
    "data = DataBunch(trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 16\n",
      "16 32\n",
      "32 64\n",
      "64 128\n"
     ]
    }
   ],
   "source": [
    "layers = [6,16,32,64,128]\n",
    "for x,y in zip(layers[0::1], layers[1::1]):\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(i, o, d, k=3, s=1):\n",
    "    #formula =>  output = input + 2*padding - kernel_size - (kernel_size-1)*(dilation-1)\n",
    "    return ((o-1)*s + (k-1)*(d-1) + k - i)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gated_Activation_Unit(Module):\n",
    "    # w/ residual connection and batchnorm\n",
    "    def __init__(self, in_c, out_c, d, k=3):\n",
    "        self.conv1 = nn.Conv1d(in_c, out_c, k, padding=d, dilation=d) # padding equals dilation to maintain size\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.conv2 = nn.Conv1d(in_c, out_c, k, padding=d, dilation=d)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.conv3 = nn.Conv1d(out_c, out_c, 1)\n",
    "        self.bn = nn.BatchNorm1d(out_c)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        tanh = self.tanh(self.conv1(x))\n",
    "        sigm = self.sigm(self.conv2(x))\n",
    "        x = self.conv3(tanh * sigm)\n",
    "        return self.bn(res + x)  #re-add residual\n",
    "        \n",
    "    \n",
    "class CBR(Module):\n",
    "    def __init__(self, in_c, out_c, k=3):\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(in_c, out_c, k, padding=k//2),\n",
    "            nn.BatchNorm1d(out_c),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(Module):\n",
    "    # without the skip connections\n",
    "    def __init__(self, in_dim=1, output_dim=11):\n",
    "        self.cbr1 = CBR(in_dim,64)\n",
    "        self.block1 = Gated_Activation_Unit(64,16,12)\n",
    "        self.block2 = Gated_Activation_Unit(16,32,8)\n",
    "        self.block3 = Gated_Activation_Unit(32,64,4)\n",
    "        self.block4 = Gated_Activation_Unit(64,128,1)\n",
    "        self.cbr2 = CBR(128,32)\n",
    "        self.out = nn.Linear(32,output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cbr1(x)\n",
    "        x = self.block4(self.block3(self.block2(self.block1(x))))\n",
    "        x = self.cbr2(x)\n",
    "        # dropout(0.2)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WaveNet(6,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Deep-Channel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset = train.values.astype('float64')\n",
    "idataset = dataset[:, 2].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "timep = dataset[:, 0]\n",
    "timep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maxer = np.amax(dataset[:, 2])\n",
    "maxchannels = maxer.astype('int')\n",
    "maxchannels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scale dataset between 0 and 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "dataset.min(), dataset.max(), dataset.mean(), dataset.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_size = int(len(dataset) * 0.80)\n",
    "modder = math.floor(train_size/batch_size)\n",
    "train_size = int(modder*batch_size)\n",
    "test_size = int(len(dataset) - train_size)\n",
    "modder = math.floor(test_size/batch_size)\n",
    "test_size = int(modder*batch_size)\n",
    "\n",
    "print(f'training set = {train_size}')\n",
    "print(f'test set = {test_size}')\n",
    "print(f'total length = {test_size + train_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_train = dataset[:, 1]\n",
    "y_train = idataset[:]\n",
    "x_train = x_train.reshape((len(x_train), 1))    #(5000000, 1)\n",
    "y_train = y_train.reshape((len(y_train), 1))    #(5000000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# SMOTE - Synthetic Minority Over-sampling Technique -> resample all classes to equal the majority class\n",
    "sm = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_res, Y_res = sm.fit_sample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Counter(y_train[:,0]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Counter(Y_res).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encode a tensor => for use w/ categorical cross-entropy \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "yy_res = Y_res.reshape((len(Y_res), 1))\n",
    "yy_res = to_categorical(yy_res, num_classes=maxchannels+1)\n",
    "xx_res, yy_res = shuffle(X_res, yy_res)  # x: (13641672, 1), y: (13641672, 1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainy_size = int(len(xx_res) * 0.80)\n",
    "modder = math.floor(trainy_size/batch_size)\n",
    "trainy_size = int(modder*batch_size)\n",
    "testy_size = int(len(xx_res) - trainy_size)\n",
    "modder = math.floor(testy_size/batch_size)\n",
    "testy_size = int(modder*batch_size)\n",
    "\n",
    "print('training set = ', trainy_size)\n",
    "print('test set =', testy_size)\n",
    "print('total length =', testy_size+trainy_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "in_train, in_test = xx_res[0:trainy_size,0], xx_res[trainy_size:trainy_size+testy_size, 0]\n",
    "target_train, target_test = yy_res[0:trainy_size,:], yy_res[trainy_size:trainy_size+testy_size, :]\n",
    "\n",
    "in_train = in_train.reshape(len(in_train), 1, 1, 1)\n",
    "in_test = in_test.reshape(len(in_test), 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "in_train.shape, in_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target_train.shape, target_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "newmodel = Sequential()\n",
    "timestep = 1\n",
    "input_dim = 1\n",
    "newmodel.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'),\n",
    "                             input_shape=(None, timestep, input_dim)))\n",
    "newmodel.add(TimeDistributed(MaxPooling1D(pool_size=1)))\n",
    "newmodel.add(TimeDistributed(Flatten()))\n",
    "\n",
    "newmodel.add(LSTM(256, activation='relu', return_sequences=True))\n",
    "newmodel.add(BatchNormalization())\n",
    "newmodel.add(Dropout(0.2))\n",
    "\n",
    "newmodel.add(LSTM(256, activation='relu', return_sequences=True))\n",
    "newmodel.add(BatchNormalization())\n",
    "newmodel.add(Dropout(0.2))\n",
    "\n",
    "newmodel.add(LSTM(256, activation='relu'))\n",
    "newmodel.add(BatchNormalization())\n",
    "newmodel.add(Dropout(0.2))\n",
    "\n",
    "newmodel.add(Dense(maxchannels+1))\n",
    "newmodel.add(Activation('softmax'))\n",
    "\n",
    "newmodel.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.001, momentum=0.9, nesterov=False),\n",
    "                 metrics=['accuracy', Precision(), Recall(), F1Score(num_classes=maxchannels+1, average='micro')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Simple 2-layer RNN from signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seq_len = 4000\n",
    "bs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = train['signal'].values.reshape(-1, seq_len, 1)\n",
    "y = train['open_channels'].values.reshape(-1, seq_len)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def trn_val_split(rows, percent=0.2):\n",
    "    \"returns trn_idxs, val_idxs\"\n",
    "    indices = np.random.permutation(rows)  # randomly permute rows\n",
    "    num = int(rows*percent)\n",
    "    return indices[num:], indices[:num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_idx,val_idx = trn_val_split(X.shape[0])\n",
    "\n",
    "trn_x, val_x = X[trn_idx], X[val_idx]\n",
    "trn_y, val_y = Y[trn_idx], Y[val_idx]\n",
    "print(trn_x.shape, val_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### normalize training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_mean = trn_x.mean()\n",
    "trn_sigma = trn_x.std()\n",
    "\n",
    "trn_x = (trn_x - trn_mean)/trn_sigma\n",
    "val_x = (val_x - trn_mean)/trn_sigma\n",
    "\n",
    "print(trn_x.mean(), trn_x.std())\n",
    "print(val_x.mean(), val_x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Ion_Dataset(Dataset):\n",
    "    def __init__(self, X, Y=None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X[idx], dtype=torch.float)\n",
    "        y = torch.tensor(self.Y[idx], dtype=torch.long) if self.Y else None\n",
    "        return (x,y) if y else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_ds = Ion_Dataset(trn_x, trn_y)\n",
    "val_ds = Ion_Dataset(val_x, val_y)\n",
    "\n",
    "trn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=bs, shuffle=False)\n",
    "\n",
    "db = DataBunch(trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Model & Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Bi_RNN(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=32, output_dim=11, num_layers=2):\n",
    "        super(Bi_RNN, self).__init__()\n",
    "\n",
    "        self.linear_in = nn.Linear(input_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.linear_out = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        linear_input = self.linear_in(input)\n",
    "        lstm_out, self.hidden = self.lstm(linear_input)\n",
    "        return self.linear_out(lstm_out)\n",
    "\n",
    "model = Bi_RNN()\n",
    "loss = CrossEntropyFlat()\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1(preds, targs):\n",
    "    return tensor(f1_score(targs.view(-1).detach().cpu(), preds.argmax(-1).view(-1).detach().cpu(), average='macro'))\n",
    "\n",
    "learn = Learner(db, model, loss_func=loss, metrics=[f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# learn.clip_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(5, lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Seq Databunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SeqProcessor():\n",
    "    def __init__(self, ds:Collection=None, seq_len=5000):  self.seq_len = seq_len\n",
    "    def process(self, ds:Collection):  ds.items = ds.items.reshape(-1, seq_len, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SeqBatchSampler(torch.utils.data.Sampler):\n",
    "    \"split data up into n batches of seq_len\"\n",
    "    def __init__(self,data_source,seq_len):\n",
    "        assert len(data_source)%seq_len==0\n",
    "        self.seq_len = seq_len\n",
    "        self.data_source = data_source.reshape(-1, seq_len, 1)\n",
    "\n",
    "    def __len__(self): return len(self.data_source)\n",
    "    def __iter__(self): return iter(self.data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d = ItemList.from_df(train, PATH, cols=1, processor=SeqProcessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def custom_collate(data):\n",
    "    x,y = [],[]\n",
    "    for ll in data:\n",
    "        x.append(ll.x.items)\n",
    "        y.append(ll.y.items)\n",
    "    return (torch.Tensor(x), torch.LongTensor(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SeqDataBunch(DataBunch):\n",
    "    @classmethod      # cls => databunch Class\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, path='.',bs:int=30, seq_len:int=5000, **kwargs) -> DataBunch:\n",
    "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "        dataloaders = []\n",
    "        for ds in datasets:\n",
    "            samp = SeqBatchSampler(ds,seq_len=seq_len)\n",
    "            dataloaders.append(DataLoader(ds, batch_size=bs, sampler=samp))\n",
    "        return cls(*dataloaders, path=path, collate_fn=custom_collate)\n",
    "    \n",
    "    def normalize(self, mean=0., std=1.)->None:\n",
    "        self.add_tfm(partial(self.norm_x, mean=mean, std=std))\n",
    "        return self\n",
    "    \n",
    "    @staticmethod\n",
    "    def norm_x(batch:Tuple[Tensor,Tensor], mean:float, std:float)->Tuple[Tensor,Tensor]:\n",
    "        x,y = batch\n",
    "        x = (x-mean)/std\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SeqList(ItemList): \n",
    "    _processor = [SeqProcessor]\n",
    "    #_bunch = SeqDataBunch\n",
    "\n",
    "    def analyze_pred(self, pred:Tensor):\n",
    "        return torch.argmax(pred, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "1000*.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.arange(len(train)).reshape(-1,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.randint(1000, size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idxs = np.arange(len(train)).reshape(-1,5000)[:,-int(0.2):].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def valid_idxs(df,seq_len=5000,percent=0.2):\n",
    "    full = np.arange(len(df)).reshape(-1,seq_len)\n",
    "    rows = full.shape[0]\n",
    "    num = int(rows*percent)\n",
    "    idx = np.random.randint(rows, size=num)\n",
    "    return full[idx].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "v_idxs = valid_idxs(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "v_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bs=100\n",
    "bptt=100\n",
    "\n",
    "v_idxs = valid_idxs(train, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = (SeqList.from_df(train, PATH, cols=1)\n",
    "        .split_by_idx(v_idxs)\n",
    "        .label_from_df(cols=2, label_cls=SeqList)\n",
    "        .databunch(bs=30)\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d = data.databunch(bs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = data.normalize(mean=1.5597, std=3.4347)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Mixed Sequential DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# class NormProcessor(PreProcessor):\n",
    "#     def __init__(self, ds:Collection=None):\n",
    "#         self.means = ds.items.mean()\n",
    "#         self.stds = ds.items.std()\n",
    "\n",
    "#     def process_one(self, item:Any): return self.norm(item)\n",
    "#     def process(self, ds:Collection): ds.items = self.norm(ds.items)\n",
    "    \n",
    "#     def norm(self, x): return (x - self.means)/(self.stds + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SeqBatchSampler(torch.utils.data.Sampler):\n",
    "    \"Return batches of indexes ~> [bs,bptt]\"\n",
    "    def __init__(self,data_source,bs=10,bptt=5000):\n",
    "        self.length = len(data_source)\n",
    "        assert self.length%bs==0\n",
    "        self.bs,self.bptt = bs,bptt\n",
    "        self.idxs = np.arange(self.length).reshape((bs,-1))\n",
    "\n",
    "    def __len__(self): return self.length//self.bs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.cur_idx = 0\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "        cur = self.cur_idx\n",
    "        end = cur+self.bptt\n",
    "        self.cur_idx = end\n",
    "        res = self.idxs[:,cur:end]\n",
    "        if res.shape[1] == 0: raise StopIteration\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def custom_collate(data):\n",
    "    x,y = [],[]\n",
    "    for ll in data:\n",
    "        x.append(ll.x.items)\n",
    "        y.append(ll.y.items)\n",
    "    return (torch.Tensor(x), torch.LongTensor(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SeqDataBunch(DataBunch):\n",
    "    @classmethod      # cls => databunch Class\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, path='.', bs:int=10, bptt:int=5000, **kwargs) -> DataBunch:\n",
    "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "        dataloaders = []\n",
    "        for ds in datasets:\n",
    "            samp = SeqBatchSampler(ds,bs=bs,bptt=bptt)\n",
    "            dataloaders.append(DataLoader(ds, batch_sampler=samp))\n",
    "        return cls(*dataloaders, path=path, collate_fn=custom_collate)\n",
    "    \n",
    "    def normalize(self, mean=0., std=1.)->None:\n",
    "        self.add_tfm(partial(self.norm_x, mean=mean, std=std))\n",
    "        return self\n",
    "    \n",
    "    @staticmethod\n",
    "    def norm_x(batch:Tuple[Tensor,Tensor], mean:float, std:float)->Tuple[Tensor,Tensor]:\n",
    "        x,y = batch\n",
    "        x = (x-mean)/std\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SeqList(ItemList): \n",
    "    #_processor = [NormProcessor]\n",
    "    _bunch = SeqDataBunch\n",
    "\n",
    "    def analyze_pred(self, pred:Tensor):\n",
    "        return torch.argmax(pred, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def valid_idxs(df,bs=10,percent=0.2):\n",
    "    length = len(df)\n",
    "    pct = length/bs*percent\n",
    "    assert pct.is_integer()\n",
    "    return np.arange(length).reshape(bs,-1)[:,-int(pct):].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bs=100\n",
    "bptt=100\n",
    "\n",
    "v_idxs = valid_idxs(train, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = (SeqList.from_df(train, PATH, cols=1)\n",
    "        .split_by_idx(v_idxs)\n",
    "        .label_from_df(cols=2)\n",
    "        .databunch(bs=bs, bptt=bptt)\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = data.normalize(mean=1.5597, std=3.4347)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Visualize (unnormalized) training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x,y = next(iter(data.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "step = 100\n",
    "seq = bptt//step\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(x.numpy().flatten()[::step])\n",
    "plt.plot(y.numpy().flatten()[::step])\n",
    "\n",
    "plt.vlines(range(0,bs*seq,seq),-5,14,'r')  # delineate batches (vertically from -5 to 12.5)\n",
    "for j in range(bs): plt.text(j*seq+(seq/2),13,str(j+1),size=20)  # show batch numbers\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Visualize (normalized) training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = data.normalize(mean=1.5597, std=3.4347)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x,y = next(iter(data.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "step = 100\n",
    "seq = bptt//step\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(x.numpy().flatten()[::step])\n",
    "plt.plot(y.numpy().flatten()[::step])\n",
    "\n",
    "plt.vlines(range(0,bs*seq,seq),-5,14,'r')  # delineate batches (vertically from -5 to 12.5)\n",
    "for j in range(bs): plt.text(j*seq+(seq/2),13,str(j+1),size=20)  # show batch numbers\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# CNN + AWD_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ConvToLSTM(Module):\n",
    "    def __init__(self, in_sz):\n",
    "        self.conv = nn.Conv1d(1,in_sz,3,padding=1)\n",
    "        \n",
    "    def forward(self, input:Tensor):\n",
    "        return self.conv(input.unsqueeze(1)).permute(0,2,1)\n",
    "\n",
    "class mod_AWD_LSTM(Module):\n",
    "    def __init__(self, in_sz:int, hid_sz:int, n_layers:int, hidden_p:float=0.2,\n",
    "                 input_p:float=0.6, weight_p:float=0.5, bidir:bool=False):\n",
    "        self.bs,self.in_sz,self.hid_sz,self.n_layers = 1,in_sz,hid_sz,n_layers\n",
    "        self.n_dir = 2 if bidir else 1\n",
    "\n",
    "        self.rnns = [nn.LSTM(self.in_sz if l == 0 else self.hid_sz, self.hid_sz//self.n_dir, 1,\n",
    "                             batch_first=True, bidirectional=bidir) for l in range(n_layers)]\n",
    "        self.rnns = [WeightDropout(rnn, weight_p) for rnn in self.rnns]\n",
    "        self.rnns = nn.ModuleList(self.rnns)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input:Tensor)->Tuple[Tensor,Tensor]:\n",
    "        bs,sl,emb = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(input)\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers-1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "        self.hidden = to_detach(new_hidden, cpu=False)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def _one_hidden(self, l:int)->Tensor:\n",
    "        \"Return one hidden state.\"\n",
    "        nh = self.hid_sz // self.n_dir\n",
    "        return one_param(self).new(self.n_dir, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]\n",
    "\n",
    "def get_model(in_sz=32, hid_sz=64, out_sz=11, n_layers=3, out_p=0.4, **kwargs):\n",
    "    return SequentialRNN(\n",
    "        ConvToLSTM(in_sz),\n",
    "        mod_AWD_LSTM(in_sz,hid_sz,n_layers,**kwargs),\n",
    "        LinearDecoder(out_sz, hid_sz, output_p=out_p)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1(preds, targs):\n",
    "    return tensor(f1_score(targs.view(-1).detach().cpu(), preds.argmax(-1).view(-1).detach().cpu(), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn = LanguageLearner(data, model, metrics=[f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# learn.model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tabular Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastai.tabular import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tab_data = TabularDataBunch.from_df(PATH, train[['signal','open_channels']], 'open_channels',\n",
    "                                    valid_idx=range(4000000, 5000000))#, procs=[Normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn = tabular_learner(tab_data, layers=[32,64], metrics=[accuracy, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save('tab_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# F1 evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "*positive class*: minority class(es)  \n",
    "*negative class*: majority class\n",
    "\n",
    "- Precision: number of true positive class predictions.  (minority class accuracy)\n",
    "- Recall: number of positive class predictions made out of all positive examples in the dataset. (minority class coverage)\n",
    "- F-Measure provides a single score that balances both the concerns of precision and recall in one number.\n",
    "\n",
    "*precision* - TruePositives / (TruePositives + FalsePositives)  \n",
    "*recall* - TruePositives / (TruePositives + FalseNegatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "| *Binary*        | Positive Prediction | Negative Prediction |\n",
    "| :-------------: | ------------------- | ------------------- |\n",
    "|**Positive Class** | True Positive (TP) | False Negative (FN) |\n",
    "|**Negative Class** | False Positive (FP) | True Negative (TN) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TP = (preds * y).sum(); TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prec = TP / preds.sum(); prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rec = TP / y.sum(); rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f1 = (prec*rec)/(prec+rec+1e-9)*2; f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TP = (y_pred*y_true).sum()\n",
    "prec = TP/(y_pred.sum(dim=1)+eps)\n",
    "rec = TP/(y_true.sum(dim=1)+eps)\n",
    "res = (prec*rec)/(prec*beta2+rec+eps)*(1+beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model,x,y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        res = model(x)\n",
    "        preds = res.argmax(dim=-1)\n",
    "        f1\n",
    "learn.model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_1.4] *",
   "language": "python",
   "name": "conda-env-pytorch_1.4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
